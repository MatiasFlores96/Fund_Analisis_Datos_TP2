---
title: "Trabajo Practico N°2"
author: "Ing. Flores, Matias & Ing. Loiseau, Matias"
format: pdf
editor: visual
---

```{r, message=FALSE, warning=FALSE}
library(rstudioapi)
library(readxl)
library(ggplot2)
library(tidyverse)
library(GGally)
library(ggthemes)
library(effectsize)
library(car)
library("nortest")
library(tseries)
library(lattice)
library(caret)
```

# Ejercicio N°1

## Punto 1

### Carga del Data Set

```{r}
setwd(dirname(getActiveDocumentContext()$path))
```

```{r, message=FALSE, warning=FALSE}
MedidasCorporales <- read_excel("MedidasCorporales.xlsx")
attach(MedidasCorporales)
```

Ponemos esto para saber cuantos registros y variables hay. Y si son todas numericas

```{r}
str(MedidasCorporales)
```

Verificamos que no haya nulos en el dataset

```{r}
which(is.na(MedidasCorporales))

```

## Punto 2

```{r}
ggplot(data = MedidasCorporales, aes(x = Altura, y = Peso)) +

  geom_point(col = "deepskyblue4", size=1.5) +
  labs(
    x = "Altura",
    y = "Peso")+
  theme_classic() +
  theme(
    plot.title = element_text(color = "black", size = 12, face = "bold", hjust = 0.5),
    panel.grid.major = element_line(color = "#6C7B8B",
                                    linewidth = 0.1,
                                    linetype = 8))
```

```{r}
cor(Altura, Peso)
```

Realizamos el modelo de regresion para ver la influencia de la altura en el peso.

```{r}
reg_lineal <- lm(formula = Peso ~ Altura, 
                  data = MedidasCorporales)
reg_lineal
```

Coeficientes de regresion

```{r}
reg_lineal$coefficients
```

```{r}
ggplot(data = MedidasCorporales, aes(x = Altura, y = Peso)) +

  geom_point(col = "deepskyblue4", size=1.5) +
  geom_abline(slope = 1.018, intercept = -105.011, color = "blue") +
  labs(
    x = "Altura",
    y = "Peso")+
  theme_classic() +
  theme(
    plot.title = element_text(color = "black", size = 12, face = "bold", hjust = 0.5),
    panel.grid.major = element_line(color = "#6C7B8B",
                                    linewidth = 0.1,
                                    linetype = 8))
```

Analizamos la regresión

```{r}
summary(reg_lineal)
```

Los coeficientes de regresion son muy significativos.

```{r}
residuos <- data.frame(Error = reg_lineal$residuals)
shapiro.test(residuos$Error)

ggplot(data = residuos, aes(x = Error)) + 
  geom_histogram(color = "black", fill = "deepskyblue3") +
  labs(
    x = "Error",
    y="") +   
  theme_classic() +
  theme(
    plot.title = element_text(color = "black", size = 16, face = "bold", hjust = 0.5),
    panel.grid.major = element_line(color = "#6C7B8B",linewidth = 0.1,linetype = 8))
```

No se aprecia normalidad en los residuos.

## Punto 3

### División del Dataset en Entrenamiento y Prueba

```{r}
n = 507
set.seed(1234)
```

```{r}
muestras <- 1:n %>% 
  createDataPartition(p=0.80, list=FALSE)
```

```{r}

entrenamiento_tp <- MedidasCorporales[muestras,]
prueba_tp <- MedidasCorporales[-muestras,]
```

```{r}
entrenamiento_tp
```

## Modelo 1

Aplicamos regresion multiple para analizar la influencia de todas las variables con Peso

```{r}
reg_multiple = lm(formula=Peso ~ ., data = entrenamiento_tp)
```

```{r}
reg_multiple
```

Verificamos los que tienen un nivel de confianza mayor al 95%

```{r}
confint(reg_multiple , level=0.95)
```

## Modelo 2

Nuevo modelo de regresión con los que tienen mas de 95%

```{r}
reg_multiple_nueva = lm(formula=Peso ~ `Profundidad de pecho` + `Diámetro de rodilla` + `Contorno de pecho` + `Contorno de cintura`+
                          `Contorno de cadera` + `Contorno de muslo` + `Contorno de antebrazo` + `Contorno de rodilla` + 
                          `Contorno de pantorrilla` + Edad + Altura, data = entrenamiento_tp)
```

Resultados del primer modelo

```{r}
summary(reg_multiple)
```

Predicciones del primer modelo

```{r}
options(width = 80) # (mejora visual de la salida)

predicciones1 <- predict(reg_multiple, prueba_tp, type = "response")

predicciones1
```

Resultados del segundo modelo

```{r}
summary(reg_multiple_nueva)
```

Predicciones del segundo modelo

```{r}
options(width = 80) # (mejora visual de la salida)

predicciones2 <- predict(reg_multiple_nueva, prueba_tp, type = "response")

predicciones2
```

Dataframe con las predicciones y reales para el modelo 1

```{r}
data1 <- data.frame(pred = predicciones1, actual = prueba_tp$Peso)
data1



```

Dataframe con las predicciones y reales para el modelo 2

```{r}
data2 <- data.frame(pred = predicciones2, actual = prueba_tp$Peso)
data2
```

MSE de ambos modelos.

```{r}
MSE_Modelo1 = mean((data1$actual - data1$pred)^2)
MSE_Modelo2 = mean((data2$actual - data2$pred)^2)
MSE_Modelo1
MSE_Modelo2
```

# Ejercicio N°2

## Punto 1

Carga del dataset

```{r, message=FALSE, warning=FALSE}
Dolor <- read_excel("Dolor.xlsx")
attach(Dolor)
```

Eliminación de nulos

```{r, message=FALSE, warning=FALSE}
Dolor_Nuevo <- drop_na(Dolor)
attach(Dolor_Nuevo)
```

```{r}
nrow(Dolor_Nuevo)
```

```{r}
which(is.na(Dolor))

```

```{r}
str(Dolor_Nuevo)
```

```{r}
df2<-Dolor_Nuevo[!(Dolor_Nuevo$Colesterol=="NA"),]
df2
```

Convertimos a la variable Colesterol en numerica

```{r}
df2$Colesterol <- as.numeric(as.character(df2$Colesterol))

```

```{r}
df2
```

Modelo de Regresión Logistica

```{r}
log_simple <- glm(formula = `Estrechamiento arterias coronarias` ~ Colesterol,
                  data = df2,
                  family = "binomial")
summary(log_simple)
```

Probabilidad de que una persona con 199 de Colesterol tenga Estrechamiento de alguna Arteria

```{r}
nuevo_valor <- data.frame(Colesterol = c(199))
prob <- log_simple %>% predict(nuevo_valor, type = "response")
prob
```

## Punto 2

```{r}
log_simple_multi <- glm(formula = `Estrechamiento arterias coronarias` ~ Edad + `Días con síntomas` + Colesterol,
                  data = df2,
                  family = "binomial")
summary(log_simple_multi)
```

Determinamos que Días con Sintomas no tiene significancia en el modelo, mientras que Edad y Colesterol tienen una fuerte significancia.

## Punto 3

Dividimos el dataset en Masculino y Femenino

```{r}

Masculino <- df2 %>%
  filter(Sexo == 0)

Femenino <- df2 %>%
  filter(Sexo == 1)
```

```{r}
log_simple_multi_M <- glm(formula = `Estrechamiento arterias coronarias` ~ Edad + `Días con síntomas` + Colesterol,
                  data = Masculino,
                  family = "binomial")
summary(log_simple_multi_M)
```

```{r}
log_simple_multi_F <- glm(formula = `Estrechamiento arterias coronarias` ~ Edad + `Días con síntomas` + Colesterol,
                  data = Femenino,
                  family = "binomial")
summary(log_simple_multi_F)
```

# Ejercicio N°3

## Punto 1

```{r, message=FALSE, warning=FALSE}
Europa <- read_excel("Europa.xlsx")
attach(Europa)

```

```{r}
Europa
```

## Punto 2

```{r}
Países <- Europa %>%
  dplyr::select(Área, PBI, Inflación, `Expectativa de vida`, `Población militar`, 
                `Crecimiento de la población`, `Tasa de desempleo`)
```

```{r}
Sigma_P = cov(Países)
Sigma_P
```

```{r}
Inv = det(Sigma_P)
Inv
```

Como es distinta a 0 es inversible.

## Punto 3

```{r}
autovalores_p <- eigen(Sigma_P)$values
autovalores_p
```

## Punto 4

```{r}
options(width = 80) # (mejora visual de la salida)

PC_p <- prcomp(Países, scale = TRUE)
PC_p$rotation
```

```{r}
options(width = 80) # (mejora visual de la salida)

summary(PC_p)

```

## Punto 5

Contribucion de variables.

```{r}
library(factoextra)
fviz_pca_var(PC_p, repel = TRUE,  
             col.var = "contrib", # según la contribución
             gradient.cols = "Set1",
             title = "",
             legend.title = "") 
```

```{r}
# Usando las dos primeras componenes prncipales
fviz_cos2(PC_p, choice = "var", axes = 1:2, fill = "pink", color = "violet") +
  ggtitle("Calidad en la representación de las variables en las dimensiones 1-2") +
  theme(plot.title = element_text(hjust = 0.5))
```

# Ejercicio N°4

## Punto 1

```{r, message=FALSE, warning=FALSE}
data("JohnsonJohnson")

```

```{r}
JohnsonJohnson
```

```{r}
help("JohnsonJohnson")
```

El dataset contiene las ganancias en dolares trimestrales de Johnson & Johnson, desde el año 1960 hasta 1980.

```{r}
class(JohnsonJohnson)
```

```{r}
length(JohnsonJohnson)
```

## Punto 2

```{r}
plot.ts(JohnsonJohnson, col ="royalblue", xlab = "Año", ylab = "Ganancias")
```

### Descomposicion Aditiva

```{r}
ganancias_da <- decompose(JohnsonJohnson, type = "additive")
```

#### Tendencia

```{r}
plot.ts(ganancias_da$trend, col = "royalblue", xlab = "Año", ylab = "Ganancias")
```

```{r}
varR <- var(na.omit(ganancias_da$random))
varTR <- var(na.omit(ganancias_da$trend) + na.omit(ganancias_da$random))
1 - varR / varTR
```

Tiene una fuerte tendencia porque se desvía hacia arriba con un fuerza de 0.97.

#### Estacionalidad

```{r}
plot.ts(ganancias_da$seasonal, col = "royalblue", xlab = "Año", ylab = "Ganancias")
```

```{r}
varR <- var(na.omit(ganancias_da$random))
varSR <- var(na.omit(ganancias_da$seasonal) + na.omit(ganancias_da$random))
1 - varR / varSR
```

Se observa una débil estacionalidad. Tiene una fuerza de 0.32.

### Descomposición Multiplicativa

```{r}
ganancias_dm <- decompose(JohnsonJohnson, type = "multiplicative")
```

#### Tendencia

```{r}
plot.ts(ganancias_dm$trend, col = "royalblue", xlab = "Año", ylab = "Ganancias")
```

```{r}
varR <- var(na.omit(ganancias_dm$random))
varTR <- var(na.omit(ganancias_dm$trend) * na.omit(ganancias_dm$random))
1 - varR / varTR
```

#### Estacionalidad

```{r}
plot.ts(ganancias_dm$seasonal, col = "royalblue", xlab = "Año", ylab = "Ganancias")
```

```{r}
varR <- var(na.omit(ganancias_dm$random))
varSR <- var(na.omit(ganancias_dm$seasonal) * na.omit(ganancias_dm$random))
1 - varR / varSR
```

## Punto 3

### BOXCOX

```{r, message=FALSE, warning=FALSE}
library(fpp2)
library(gridExtra)
```

```{r, message=FALSE, warning=FALSE}
pp.test(JohnsonJohnson)
```

```{r, message=FALSE, warning=FALSE}
kpss.test(JohnsonJohnson)
```

APLICAMOS BOXCOX para intentar estacionar

```{r}
lamb <- BoxCox.lambda(JohnsonJohnson)
lamb
```

```{r}
JohnBC <- BoxCox(JohnsonJohnson, lambda = lamb)
```

```{r}
sto <- autoplot(JohnsonJohnson, color = "royalblue", main =  "Serie de tiempo original", xlab = "", ylab = "") +
  theme_hc() +
  theme(plot.title = element_text(hjust = 0.5))

stt <- autoplot(JohnBC, color = "royalblue", main =  "Serie de tiempo transformada", xlab = "", ylab = "") +
  theme_hc() +
  theme(plot.title = element_text(hjust = 0.5))

grid.arrange(sto, stt, ncol = 1)
```

```{r}
JohnBC
```

Dividimos en entrenamiento y prueba

```{r}
entrenamiento_john <- window(JohnBC, end = c(1960,76))
prueba_john <- window(JohnBC, start = c(1977,9))
prueba_john
```

```{r, message=FALSE, warning=FALSE}
PP.test(JohnBC)
```

```{r, message=FALSE, warning=FALSE}
kpss.test(JohnBC)
```

Determinamos que sigue sin ser estacionaria

Aplicamos diferenciacion para eliminar la tendencia y estacionar.

```{r}
JOHN_dif <- diff(JohnBC)
autoplot(JOHN_dif, color = "royalblue", xlab = "Año", ylab = "Ganancias diferenciadas")
```

```{r}
Acf(JOHN_dif, lag.max = 14, type = "partial", main = "") 
```

```{r}
Acf(JOHN_dif, lag.max = 30, type = "correlation", main = "") 
```

Haciendo diff 1 nos da estacionario

```{r}
pp.test(JOHN_dif)
```

```{r}
kpss.test(JOHN_dif)
```

```{r}
entrenamiento_john <- window(JohnBC, end = c(1960,76))
prueba_john <- window(JohnBC, start = c(1977,9))
prueba_john
```

Hacemos diff en entrenamiento y prueba

```{r}
entrenamiento_john_diff <- diff(entrenamiento_john)
prueba_john_diff <- diff(prueba_john)
```

Mejor eleccion de p y justificacion

```{r}
options(width = 80) # mejora la visual de la salida

AIC_AR <- vector()
for (p in 1:14){
  modelo <- arima(entrenamiento_john_diff, order = c(p,0,0))
  AIC_AR[p] <- modelo$aic
}
AIC_AR
```

```{r}
AIC_AR <- data.frame(Órden = 1:14, AIC = AIC_AR)
minAIC_AR <- filter(AIC_AR, AIC == min(AIC))
minAIC_AR

ggplot(AIC_AR, aes(x = Órden, y = AIC)) +
  geom_line(color = "royalblue") +
  geom_point(data = minAIC_AR, aes(x = Órden, y = AIC), size = 2, color = "#6CA6CD")
```

Mejor eleccion de q y justificacion

```{r}
options(width = 80) # mejora la visual de la salida

AIC_MA <- vector()
for (q in 1:30){
  modelo <- arima(entrenamiento_john_diff, order = c(0,0,q))
  AIC_MA[q] <- modelo$aic
}
AIC_MA
```

```{r}
AIC_MA <- data.frame(Órden = 1:30, AIC = AIC_MA)
minAIC_MA <- filter(AIC_MA, AIC == min(AIC))
minAIC_MA

ggplot(AIC_MA, aes(x = Órden, y = AIC)) +
  geom_line(color = "royalblue") +
  geom_point(data = minAIC_MA, aes(x = Órden, y = AIC), size = 2, color = "#6CA6CD")
```

```{r}
ARIMA <- arima(entrenamiento_john_diff, order = c(13,0,4), method = "ML")
round(summary(ARIMA)$coef, 4)
```

```{r}
predARIMA <- forecast(ARIMA, h = 8)
predARIMA
```

```{r}
options(width = 80) # mejora la visual de la salida

accuracy(predARIMA, prueba_john_diff)
```

AUTOMATICO

```{r}
autoARIMA <- auto.arima(entrenamiento_john_diff)
summary(autoARIMA)
```

```{r}
pred_autoARIMA <- forecast(autoARIMA, h = 8)
```

```{r}
options(width = 80) # mejora la visual de la salida

accuracy(pred_autoARIMA, prueba_john_diff)
```

## Punto 6

```{r}
summary(ARIMA)
```

```{r}
summary(autoARIMA)
```

```{r}
ARIMA_Nuestro <- arima(entrenamiento_john_diff, order = c(16,0,5), method = "ML")
round(summary(ARIMA_Nuestro)$coef, 4)
```

```{r}
pred_ARIMANuestro <- forecast(ARIMA_Nuestro, h = 8)
pred_ARIMANuestro
```

```{r}
options(width = 80) # mejora la visual de la salida

accuracy(pred_ARIMANuestro, prueba_john_diff)
```

```{r}
summary(ARIMA_Nuestro)
```

Ultimo Año

```{r}
#entrenamiento_john_2 <- window(JohnBC, end = c(1960,76))
prueba_john_2 <- window(JohnBC, start = c(1980,1))
prueba_john_diff_2 <- diff(prueba_john_2)

```

```{r}

```

```{r}
accuracy(predARIMA, prueba_john_diff_2)
accuracy(pred_autoARIMA, prueba_john_diff_2)
accuracy(pred_ARIMANuestro, prueba_john_diff_2)
```
